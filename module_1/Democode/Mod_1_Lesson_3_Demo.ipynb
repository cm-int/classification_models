{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/cm-int/classification_models/blob/main/module_1/Democode/Mod_1_Lesson_3_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "Etgur7vQ6b7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation of Gradient Descent from First Principles\n",
        "\n",
        "This demonstration shows an example of using differential calculus to implement gradient descent. The example comprises two dimensions, X and Y, but you can extend the same process to handle three or more dimensions. \n",
        "\n",
        "For a description of this algorithm, see the topic **How machine learning uses differential calculus for gradient descent**\n",
        "\n",
        "There are four main elements to the algorithm:\n",
        "\n",
        "* A cost function. This example calculates the Mean Squared Error (MSE)\n",
        "* A regression function that creates an estimated set of points for the Y axis using the slope, m, and the Y intercept, b \n",
        "* The partial derivative of the cost function with respect to the slope, m\n",
        "* The partial derivative of the cost function with respect to the y-intercept, b"
      ],
      "metadata": {
        "id": "cnAQv5pLn_di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean squared error (cost function):\n",
        "\n",
        "$$C=\\frac{1}{n}\\sum_{i=1}^{n}\\left({\\hat{y}}_i-y_i\\right)^2$$"
      ],
      "metadata": {
        "id": "s44mwyDlr2di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(y_hat, y):\n",
        "    n = len(y_hat)\n",
        "    diff = y_hat - y\n",
        "    diff_squared = sum(diff * diff)\n",
        "    return diff_squared / n"
      ],
      "metadata": {
        "id": "ur263bdcTwH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression equation:\n",
        "$${\\hat{y}}_i=m{\\ \\times\\ x}_i+b$$"
      ],
      "metadata": {
        "id": "CZAC0RU3sKi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def regress(m, b, x):\n",
        "    return m * x + b"
      ],
      "metadata": {
        "id": "ChnjqbqeVEW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partial derivative of cost (C) with respect to m:\n",
        "\n",
        "$$\\frac{\\partial C}{\\partial m}=\\frac{2}{n}\\sum_{i=1}^{n}x_i\\ \\times\\ \\left({\\hat{y}}_i-y_i\\right)$$"
      ],
      "metadata": {
        "id": "r2T72gc7sbvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delC_delm(x, y_hat, y):\n",
        "    s = sum(x * (y_hat - y))\n",
        "    return (2 / len(x)) * s"
      ],
      "metadata": {
        "id": "Zuok8s3BTyKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partial derivative of cost (C) with respect to b:\n",
        "\n",
        "$$\\frac{\\partial C}{\\partial b}=\\frac{2}{n}\\sum_{i=1}^{n}\\left({\\hat{y}}_i-y_i\\right)$$"
      ],
      "metadata": {
        "id": "oe-EDG3As0MX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delC_delb(y_hat, y):\n",
        "    s = sum(y_hat - y)\n",
        "    return (2 / len(y)) * s"
      ],
      "metadata": {
        "id": "h9nIA9xnT5ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample data: arrays **x** and **y**\n",
        "\n",
        "Initial guesses for slope **m**, and y-intercept, **b**\n",
        "\n",
        "Learning rate: **lr**\n",
        "\n",
        "Minimum cost threshold: **tol**. The algorithm stops when the cost falls below this level.\n",
        "\n",
        "Maximimum number of iterations to perform: **max_iters**. The algorithm halts after this number of iterations if it doesn't converge\n"
      ],
      "metadata": {
        "id": "Qga7qTl7tHIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "size = 25\n",
        "lower_x = 0\n",
        "upper_x = 50\n",
        "\n",
        "# Replace x and y with your own sample data to experiment\n",
        "x = np.array([0, 1, 2, 3, 4, 5, 6, 7.])\n",
        "y = np.array([1.86, 1.31, .62, .33, .09, -.67, -1.23, -1.37])\n",
        "\n",
        "# Gradient descent parameters. Experiment with these as well\n",
        "m = 1\n",
        "b = 0.1\n",
        "lr = 0.01 # Don't make this value too big because the algorithm might not converge\n",
        "tol = 1e-4\n",
        "max_iters = 500"
      ],
      "metadata": {
        "id": "_yBfPqK6Wdbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradient Descent##\n",
        "\n",
        "```\n",
        "Loop while number of iterations performed is less than num_iters:\n",
        "    \n",
        "    Calculate new estimates for Y based on m, b, and X using the regression function Y = m * X + b\n",
        "    \n",
        "    Find the cost of these new estimates by finding the MSE\n",
        "    \n",
        "    If the cost is less than tol, we have finish so stop\n",
        "\n",
        "    Create a new estimate for m using the partial derivative of cost with respect to m multiplied by the learning rate, lr\n",
        "\n",
        "    Create a new estimate for b using the partial derivative of cost with respect to b multiplied by the learning rate, lr\n",
        "  \n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "R3fTj7SjvIcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "plt.scatter(x, y, s=150, c='red')\n",
        "\n",
        "costs = []\n",
        "for i in range(0, max_iters):\n",
        "\n",
        "    y_hat = regress(m, b, x)\n",
        "    #plt.plot(x, m * x + b, c='lightgrey')  # Uncomment this line to see intermediate fits\n",
        "\n",
        "    cost = mse(y_hat, y)\n",
        "    costs.append(cost) # Save the costs in a list. These will be graphed later\n",
        "    if cost <= tol:\n",
        "            break\n",
        "\n",
        "    new_m = m - delC_delm(x, y_hat, y) * lr\n",
        "    new_b = b - delC_delb(y_hat, y) * lr\n",
        "\n",
        "    m = new_m\n",
        "    b = new_b\n",
        "\n",
        "plt.plot(x, m * x + b, c='black')\n",
        "plt.xlabel('X', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 20})\n",
        "plt.ylabel('Y', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 20})\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(f'Estimated line is: y = {m} * x + {b}')"
      ],
      "metadata": {
        "id": "I5hRR0slp2Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradient of Cost##\n",
        "\n",
        "Graph of cost versus the number of iterations. The cost decreases as the number of iterations increases."
      ],
      "metadata": {
        "id": "RnTowrPR20FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "costs_index = range(0, len(costs))\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "plt.plot(costs_index[::10], costs[::10])  # Plot every 10th point (stops the graph from being a mass of dots)\n",
        "plt.scatter(costs_index[::10], costs[::10])\n",
        "\n",
        "plt.xlabel('Iterations', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 20})\n",
        "plt.ylabel('Cost', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 20})\n",
        "plt.title('Gradient of Cost', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 20})\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yz6O2SZYeX2a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
