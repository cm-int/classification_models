{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/cm-int/classification_models/blob/main/module_1/Democode/Mod_1_Lesson_3_Classification_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "Etgur7vQ6b7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using Gradient Descent to Classify Data\n",
        "\n",
        "This demonstration applies the principles of gradient descent to a simple classification scenario.\n",
        "\n",
        "The sample data comprises two sets of linearly-separable coordinates, illustrated as blue and red dots on a graph. The algorithm attempts to find the best linear boundary between the two sets. \n",
        "\n",
        "![Data used to demonstrate classification](classification_data.png)\n",
        "\n",
        "The linear regression algorithm is the same as that used in the previous demonstration. The main difference is that this example minimizes the cost of dots that appear on the wrong side of an estimated linear boundary:\n",
        "\n",
        "* The *points_above* function returns the coordinates of all points that lie above the line determined by the parameters m (slope) and b (y-intercept).\n",
        "\n",
        "* The *points_below* function returns the coordinates of all points that lie below the line determined by the parameters m (slope) and b (y-intercept).\n",
        "\n",
        "The classification algorithm iterates over both sets of points to find the red points that are below the boundary and the blue points that are above the boundary. These are the points that are on the wrong side of the line. The algorithm uses the cost function to determine the MSE of these points and performs gradient descent to reduce this cost on each iteration.\n",
        "\n",
        "For a description of the gradient descent algorithm, see the topic **How machine learning uses differential calculus for gradient descent**"
      ],
      "metadata": {
        "id": "cnAQv5pLn_di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean squared error (cost function):\n",
        "\n",
        "$$C=\\frac{1}{n}\\sum_{i=1}^{n}\\left({\\hat{y}}_i-y_i\\right)^2$$"
      ],
      "metadata": {
        "id": "s44mwyDlr2di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(y_hat, y):\n",
        "    n = len(y_hat)\n",
        "    diff = y_hat - y\n",
        "    diff_squared = sum(diff * diff)\n",
        "    return diff_squared / n"
      ],
      "metadata": {
        "id": "ur263bdcTwH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression equation:\n",
        "$${\\hat{y}}_i=m{\\ \\times\\ x}_i+b$$"
      ],
      "metadata": {
        "id": "CZAC0RU3sKi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def regress(m, b, x):\n",
        "    return m * x + b"
      ],
      "metadata": {
        "id": "ChnjqbqeVEW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partial derivative of cost (C) with respect to m:\n",
        "\n",
        "$$\\frac{\\partial C}{\\partial m}=\\frac{2}{n}\\sum_{i=1}^{n}x_i\\ \\times\\ \\left({\\hat{y}}_i-y_i\\right)$$"
      ],
      "metadata": {
        "id": "r2T72gc7sbvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delC_delm(x, y_hat, y):\n",
        "    s = sum(x * (y_hat - y))\n",
        "    return (2 / len(x)) * s"
      ],
      "metadata": {
        "id": "Zuok8s3BTyKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partial derivative of cost (C) with respect to b:\n",
        "\n",
        "$$\\frac{\\partial C}{\\partial b}=\\frac{2}{n}\\sum_{i=1}^{n}\\left({\\hat{y}}_i-y_i\\right)$$"
      ],
      "metadata": {
        "id": "oe-EDG3As0MX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delC_delb(y_hat, y):\n",
        "    s = sum(y_hat - y)\n",
        "    return (2 / len(y)) * s"
      ],
      "metadata": {
        "id": "h9nIA9xnT5ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def points_above(x, y, m, b):\n",
        "    points = []\n",
        "    for i in range(1, len(x)-1):\n",
        "        y_hat = m * x[i] + b\n",
        "        if y_hat - y[i] > 0:\n",
        "          points.append((x[i], y[i]))\n",
        "    return points"
      ],
      "metadata": {
        "id": "J0j1txyfYAy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def points_below(x, y, m, b):\n",
        "    points = []\n",
        "    for i in range(1, len(x)-1):\n",
        "        y_hat = m * x[i] + b\n",
        "        if y_hat - y[i] < 0:\n",
        "          points.append((x[i], y[i]))\n",
        "    return points"
      ],
      "metadata": {
        "id": "IFuv3Y82YCWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import random\n",
        "\n",
        "xr = random.randint(low=0, high=100, size=100)\n",
        "yr = np.arange(21, 41, 0.2)\n",
        "\n",
        "xb = random.randint(low=0, high=100, size=100)\n",
        "yb = np.arange(0, 20, 0.2)"
      ],
      "metadata": {
        "id": "2AAhHPfGYsry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial guesses for slope **m**, and y-intercept, **b**\n",
        "\n",
        "Learning rate: **lr**\n",
        "\n",
        "Minimum cost threshold: **tol**. The algorithm stops when the cost falls below this level.\n",
        "\n",
        "Maximimum number of iterations to perform: **max_iters**. The algorithm halts after this number of iterations if it doesn't converge\n"
      ],
      "metadata": {
        "id": "Qga7qTl7tHIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Gradient descent parameters. Experiment with these\n",
        "m = 1\n",
        "b = 0.1\n",
        "lr = 0.0002 # Don't make this value too big because the algorithm might not converge\n",
        "tol = 1e-4\n",
        "max_iters = 30000"
      ],
      "metadata": {
        "id": "_yBfPqK6Wdbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classiciation using Gradient Descent\n",
        "\n",
        "```\n",
        "Loop while number of iterations performed is less than num_iters:\n",
        "    \n",
        "    Find all red dots that are below the line Y = m * X + b, and all blue dots that are above this line\n",
        "\n",
        "    If there are no red dots below the line and no blue dots above the line we have finished, so stop\n",
        "\n",
        "    Calculate new estimates for Y based on m, b, and X using the regression function Y = m * X + b\n",
        "    \n",
        "    Find the cost of these new estimates by finding the MSE for red dots and blue dots that are on the wrong side of the line\n",
        "    \n",
        "    If the cost is less than tol we have finished, so stop\n",
        "\n",
        "    Create a new estimate for m using the partial derivative of cost with respect to m multiplied by the learning rate, lr\n",
        "\n",
        "    Create a new estimate for b using the partial derivative of cost with respect to b multiplied by the learning rate, lr\n",
        "  \n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "R3fTj7SjvIcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.series import is_empty_data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "plt.scatter(xr, yr, s=15, c='red')\n",
        "plt.scatter(xb, yb, s=15, c='blue')\n",
        "\n",
        "costs = []\n",
        "for i in range(0, max_iters):\n",
        "\n",
        "    points = points_above(xb, yb, m, b) + points_below(xr, yr, m, b)\n",
        "    if not(len(points)):\n",
        "      break\n",
        "\n",
        "    x = np.array(list(zip(*points))[0])\n",
        "    y = np.array(list(zip(*points))[1])\n",
        "\n",
        "    y_hat = regress(m, b, x)\n",
        "\n",
        "    cost = mse(y_hat, y)\n",
        "    costs.append(cost) # Save the costs in a list. These will be graphed later\n",
        "    if cost <= tol:\n",
        "            break\n",
        "\n",
        "    new_m = m - delC_delm(x, y_hat, y) * lr\n",
        "    new_b = b - delC_delb(y_hat, y) * lr\n",
        "\n",
        "    m = new_m\n",
        "    b = new_b\n",
        " \n",
        "    if i % 2000 == 0: # Show progress every 2000 iterations\n",
        "      plt.plot(x, m * x + b, c='lightgrey')\n",
        "\n",
        "plt.plot(x, m * x + b, c='black')\n",
        "plt.xlabel('X', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 20})\n",
        "plt.ylabel('Y', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 20})\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(f'Estimated line is: y = {m} * x + {b}')"
      ],
      "metadata": {
        "id": "I5hRR0slp2Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradient of Cost##\n",
        "\n",
        "Graph of cost versus the number of iterations. The cost decreases as the number of iterations increases."
      ],
      "metadata": {
        "id": "RnTowrPR20FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "costs_index = range(0, len(costs))\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "plt.plot(costs_index[::1000], costs[::1000])  # Plot every 1000th point (stops the graph from being a mass of dots)\n",
        "plt.scatter(costs_index[::1000], costs[::1000])\n",
        "\n",
        "plt.xlabel('Iterations', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 20})\n",
        "plt.ylabel('Cost', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 20})\n",
        "plt.title('Gradient of Cost', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 20})\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yz6O2SZYeX2a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
